
\section{NCAA Basketball Dataset}

Most recent activity detection datasets like THUMOS \cite{THUMOS},
ActivityNet \cite{ActivityNet}, and others such as
\cite{UCF101,Finegrained_cooking}, mostly contain videos where a single actor is performing the activity.
By contrast, we are interested in settings where there are multiple
people in each frame, only a small number of whom are involved with
the  activity, and where we are interested in recognizing the event
and the key participants.
A natural choice for such data is multi-player sports.

In this paper, we focus on basketball games, although our techniques
are general purpose.
In particular,  we use a subset of the $296$ NCAA games available from 
YouTube\footnote{https://www.youtube.com/user/ncaaondemand}.  These games are
played in different venues over different periods of time.
We only consider the most recent $257$ games, since older games used
slightly different rules than modern basketball.

The NCAA dataset is accompanied by freeform play-by-play text snippets describing the events
at a few key-moments in each game.
Unfortunately, the temporal frequency of this annotation is too sparse
to be useful for training / testing an event detection system.
So we decided to use Amazon Mechanical Turk to annote the events in
these videos. We will release our annotated data upon paper
acceptance.

To determine the set of events, 
we clustered the
play-by-play text to identify the most frequent events from all games. We
narrowed the event list to $11$ classes,
listed in Section~\ref{sec:experiments}.
In particular, we considered 
5 types of shots, each of which could be succesful or failed,
plus a steal event. 

Next we setup an AMT task, where the
annotators were asked to annotate the ``end-point" of these events if and when
they occur in the videos. 
To determine the starting time, we assumed that each event was 4
seconds long. 
The videos are typically $1.5$ hours long.

The videos were randomly split into $212$ training, $12$ validation and $33$
test videos. 
We split each of these videos into 4 second clips (using the
annotation boundaries), and subsampled these to 6fps.
We filter out clips which are not profile shots (such as those shown in
Figure~\ref{fig:model}) using a separately trained classifier; this excludes close up shots of players, animations, as
well as crowd shots.
This resulted in a total of $11436$ training, $856$ validation
and $2256$ test clips, each of which has one of 11 labels.

Note that this is comparable in size to the THUMOS'15 detection
challenge ($XXXX$ trimmed training instances for $20$ classes and $6553$
untrimmed validation instances). The distribution of annotations across all the
different events is visualized in Fig.~\ref{fig:data_dist}, with sample videos
for a few event classes. To the best of our
knowledge, this is the first dataset with dense temporal annotations for
such long video sequences.
