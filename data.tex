\begin{table}[ht!]
\begin{center}
\small
 \begin{tabular}{|l|c|c|}
  \hline
  Event          & \# Videos Train (Test) & Avg. \# people \\ \hline \hline
  3-point succ.    & 895 (188) &  8.35 \\ 
  3-point fail.    & 1934 (401) &  8.42 \\ 
  free-throw succ. & 552 (94) &  7.21\\ 
  free-throw fail. & 344 (41) &  7.85\\  
  layup succ.      & 1212 (233) &  6.89\\ 
  layup fail.      & 1286 (254) &  6.97 \\ 
  2-point succ.    & 1039 (148) &  7.74 \\ 
  2-point fail.    & 2014 (421) &  7.97\\ 
  slam dunk succ.  & 286 (54) &  6.59 \\ 
  slam dunk fail.  & 47 (5) &  6.35\\ 
  steal & 1827 (417) & 7.05\\ \hline  
  \end{tabular}
\end{center}
  \caption{The number of videos per event in our dataset along with
  the average number of people per video corresponding to each of the
events. The number of people is higher than existing datasets for
multi-person event recognition.}
  \label{tab:data_dist}
\end{table}



\section{NCAA Basketball Dataset}
\label{sec:data}

\eat{
Most recent activity detection datasets like THUMOS \cite{THUMOS},
ActivityNet \cite{ActivityNet}, UCF101 
\cite{UCF101}, finegrained cooking \cite{Finegrained_cooking},
contain videos with a single actor performing one activity.
By contrast, we are interested in settings where there are multiple
people in each frame, only a small number of whom are involved with
the  activity. In such a setting, we are interested in recognizing the event
as well as the key participants. A natural choice for such data is
multi-player team sports.
}

A natural choice for collecting multi-person action videos is team sports.
In this paper, we focus on basketball games, although our techniques
are general purpose.
In particular,  we use a subset of the $296$ NCAA games available from 
YouTube.\footnote{https://www.youtube.com/user/ncaaondemand}  These games are
played in different venues over different periods of time.
We only consider the most recent $257$ games, since older games used
slightly different rules than modern basketball.
The videos are typically $1.5$ hours long.
We manually identified $11$ key event types
listed in Tab.~\ref{tab:data_dist}.
In particular, we considered 
5 types of shots, each of which could be successful or failed,
plus a steal event. 

Next we launched an Amazon Mechanical Turk task, where the
annotators were asked to annotate the ``end-point" of these events if and when
they occur in the videos; end-points are usually well-defined (e.g.,
the ball leaves the shooters hands and lands somewhere else, such as
in the basket).
To determine the starting time, we assumed that each event was 4
seconds long, since it is hard to get raters to agree on when an event
started. 
This give us enough temporal context to classify each event, while
still being fairly well localized in time.


The videos were randomly split into $212$ training, $12$ validation and $33$
test videos. 
We split each of these videos into 4 second clips (using the
annotation boundaries), and subsampled these to 6fps.
We filter out clips which are not profile shots (such as those shown in
Figure~\ref{fig:model}) using a separately trained classifier; this excludes close-up shots of players,  as
well as shots of the viewers and instant replays.
This resulted in a total of $11436$ training, $856$ validation
and $2256$ test clips, each of which has one of 11 labels.
Note that this is comparable in size to the THUMOS'15 detection
challenge (150 trimmed training instances for each of the $20$ classes and $6553$
untrimmed validation instances). The distribution of annotations across all the
different events is visualized in Tab.~\ref{tab:data_dist}.


In addition to annotating the event label and start/end time,
we collected AMT annotations on $850$ video clips in the test
set, where the annotators were asked to mark the position of the ball
on the frame where the shooter attempts a shot.
To the best of our
knowledge, this is the first dataset with dense temporal annotations for
such long video sequences.

%\noindent \textbf{Player detections.}
We also used AMT to annotate the bounding boxes of all the players in a
subset of 9000 frames from the training videos.
We then trained a Multibox detector \cite{Szegedy_arxiv14}
with these annotations, and ran the trained detector on all the videos in our dataset.
We retained all detections above a confidence of 0.5 per frame;
this resulted in 6--8 person detections per clip, as listed in Tab.~\ref{tab:data_dist}.
The multibox model achieves an average overlap of $0.7$ at a recall of $0.8$
with ground-truth bounding boxes in the validation videos.

We plan to release our annotated data, including time stamps, ball
location, and player bounding boxes, upon acceptance of the paper.
