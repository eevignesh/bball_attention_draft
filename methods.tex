\section{Our Method}
% -------- Heat Map for youtube videos
\begin{figure*}[t!]
\begin{center}

   \subfigure[Attention without tracks]{
                \includegraphics[width=3 in]{images/system_figure_2_cropped.pdf}
                \label{fig:heatmap_ft}
    }
   \subfigure[Attention with tracks]{
                \includegraphics[width=3 in]{images/system_figure_1_cropped.pdf}
                \label{fig:heatmap_lp}
    }
\end{center}
   \caption{Our attention models (a) without tracks and (b) with tracks are
   shown here. The variables in the model are explained in the methods
 section.}
\label{fig:att_heatmap}
\end{figure*}
% ---------------------------------------------------------------------------------


All events in a basketball game are performed in the same scene by the same set
of players. The only basis for differentiating these events is the action
pefromed by a small subset of players at a given time.  For instance, a
``steal" event is completely defined by the action of the player attempting to
pass the ball and the player stealing from him.  To understand such an event,
it is sufficient to observe only these players pariticipating in the event.

This motivates us to build a model which can reason about an event by focusing
on specific players during the different phases of the game.  Such a foucsing
mechanism would also provide valuable information about the game-state, making
the model more interpretable.

\subsection{Player detection and feature extraction}
Given a set of basketball video clips, we first preprocess the clips to extract
player bounding boxes and features. We train a Multibox model \cite{} to detect
basketball players with bounding-box annotations collected on a subset of
training video frames. This model is then run on all frames of the training and
test videos to extract player bounding boxes.

A video clip $v$ is represented by a sequence of $D_f$ dimensional
frame-features $f_t$ extracted at each time-step $t$, as well as a set of $D_p$
dimensional player features $\mathcal{P}_t$ extracted from the player bounding
boxes at time $t$.

\begin{eqnarray}
  v & = & \left[ \left(f_1, \mathcal{P}_1\right), \dots,
\left(f_T, \mathcal{P}_T\right) \right]\;,\;\;f_t \in \mathbb{R}^{D_f} \\ \nonumber
\mathcal{P}_t & = & \{ p_{t1}, \dots, p_{t{P_t}} \}\;,\;\;p_{ti} \in \mathbb{R}^{D_p} \nonumber
\end{eqnarray}whre $P_t$ is the number of boxes at time $t$ and $T$ is the number of frames
in the video.

The frame feature $f_t$ is the activation of the last fully connected layer of
GoogLeNet \cite{} trained on ImageNet \cite{}.  The player feature $p_t^j$
contains both appearance and spatial components. The appearance features were
extracted by feeding the cropped and resized player region from the frame
through GoogLeNet and spatially pooling the response from a lower layer. The
spatial feature corresponds to a spatial histogram indicating the bounding box
location.  We explain this feature representation in more detail in the
supplementary material.

\subsection{LSTM event classifier}
Given a video $v$, we pass the frame-level features $f_t$ at time $t$ through a
Bidirectional Long Short-Term Memory (BLSTM) network. This helps in combining
contextual information from adjacent frames to provide a better frame
representation.  Let the hidden state of this BLSTM at time $t$ be denoted by
$h^f_t$.  This is the concatenated hidden state from both the forward and the
reverse LSTMS of the BLSTM. Refer to supp.  material for more details.

In order to attend to specific players in each frame, we pass the player features
through our attention model to generate an attention weighted player feature
$a_t$. The attention model is the main contribution of our work
and is explained in detail in the next section.

We feed the concatenated vector $[h^f_t, a_t]$ to a final event classification
LSTM, similar to the LRCN \cite{} model for video classification. Let
the hidden state of this LSTM at time $t$ be represented by $h^e_t$.  We then
train the model by minimizing the following one-vs-all hinge loss:

\begin{equation}
  L = \sum_{k = 1}^K \max (0, 1 - y_k w_k \cdot h^e_t)^2,
\end{equation} where $y_k$ is $1$ if the video belongs to class $k$,
else it is $-1$, and the weight vector corresponding to
class $k$ is denoted by $w_k$.

\subsection{Attention models}

Unlike past attention models \cite{}, we need to attend to a different set of
features at each time-step. There are two key issues to address in this
setting.

First, although we have different set of detections in each frame, the player
detections can be connected across the frames through an object tracking
method. This could potentially lead to better feature representation of the
players.

Second, player attention depends on the state of the event and needs to evolve
with the event.  For instance, during the start of a ``free-throw" it is
important to attend to the player making the shot. However, towards the end of
the event the success or failure of the shot can be judged by observing the
person in posession of the ball.

With these issues in mind, we explore two models:
one without an object tracker and the other using an object tracker.

\noindent \textbf{Player attention without tracking}
Here, we treat the detections in each frame to be independent from other
frames.  This alows the model to be more flexible in switching attention
between players as the event progresses.  As we later observe empirically, this
leads to better interpretability.  Also, the model does not suffer from
tracking errors.

At each time-step $t$, this model selects from the set of player features
$\mathcal{P}_t$. The attention is decided by a softmax weighting:

\begin{eqnarray} \label{eq:notrack}
  a_t & = & \sum_{i=1}^{P_t} \alpha_{ti} p_{ti} \\ \nonumber
  \alpha^{ti} & = & \text{softmax} \left(\phi\left(h^f_t, p_{ti}, h^e_{t-1}\right)\right),
\end{eqnarray}where the attention value $\alpha_{ti}$ for the $i^{th}$ player
detection at $t$ is decided by a softmax over the player scores $e_{ti}$. This
score is obtained from a multi layer perceptron $\phi$, similar to
\cite{Bahdnau_arxiv14}. Refer to the supp. material for details.

\noindent \textbf{Player attention with tracking}
In this model, we first associate the detections across all the frames to
obtain tracks corresponding to each player in the video.  We use a
standard multi-object tracker with bipartite matching to obtain the tracks
Fig.\ref{fig:track_specific_model}.

The player-track is a temporal sequence representing the change in position and
appearance of the player with time.  We could use this to gain a better
representation for the player which incorporates temporal context from all
frames. Hence, we pass the features corresponding to the bounding box of the
track at each time-step through a BLSTM as shown in
Fig.~\ref{fig:track_specific_model}. Let the hidden state of this ``player"
BLSTM corresponding to the $k^{th}$ track be denoted by $h^p_{tk}$. Similar to
Eq.~\ref{eq:notrack}, the attention-weighted $a_t$ feature is given by:

\begin{eqnarray}
  a_t & = & \sum_{i=1}^{P_t} \alpha_{ti} h^p_{ti} \\ \nonumber
  \alpha^{ti} & = & \text{softmax} \left(\phi\left(h^f_t, h^p_{ti}, h^e_{t-1}\right)\right),
\end{eqnarray}

While this provides a more consistent representation for the players across frames,
this model could suffer from wrong associations due to trakcing. This in turn makes
the interpretation of attention scores more difficult.
