\section{Our Method}
\label{sec:methods}

All events in a team sport are performed in the same scene by the same set
of players. The only basis for differentiating these events is the action
pefromed by a small subset of people at a given time.  For instance, a
``steal" event in basketball is completely defined by the action of the player attempting to
pass the ball and the player stealing from him.  To understand such an event,
it is sufficient to observe only these players pariticipating in the event.

This motivates us to build a model which can reason about an event by focusing
on specific people during the different phases of the event.
In this section, we describe our unified model for classifying events
and simultaneously identifying the key players.

\begin{figure}[t!]
\begin{center}
    \includegraphics[width=3 in]{images/system_figure_1_cropped_v2.pdf}
\end{center}
   \caption{Our attention model where each player track is represented by the
     corresponding BLSTM. The variables in the model are explained in the
     methods section.  BLSTM stands for ``bidirection long short term memory''.
}
\label{fig:model}
\end{figure}

\subsection{Feature extraction}

Each video-frame is represented by a feature vector $f_t$, which is the activation
of the last fully connected layer of Inception7 network
\cite{Ioffe_arxiv15,Inception7}.  In addition, we compute spatially localized features
for each person in the frame. In particular,
$p_{ti}$ contains both appearance and spatial information for the $i$'th
player bounding box in frame $t$. Similar to the RCNN object detector\cite{},
the appearance features were extracted by feeding
the cropped and resized player region from the frame through the Inception7 network and
spatially pooling the response from a lower layer. The spatial feature
corresponds to a 32 x 32 spatial histogram, combined with a spatial
pyramid, to indicate the bounding box location at multiple scales.

\subsection{Event classification}

Given $f_t$ and $p_{ti}$ for each frame $t$ from $1$ to $T$, our goal
is to train the model to classify the clip into one of 11 categories. As a side
effect of the way we construct our model, we will also be able to identify the
key player in each frame.

First we compute a global context feature for each frame, $h_t^f$, derived from
a bidirectional LSTM applied to the frame-level feature as shown in Fig.~\ref{fig:system_fig}.
This is a concatenation of the hidden states from the forward and reverse LSTM
components of a BLSTM and can be compactly represented as:
\[
  h_t^f = \mbox{BLSTM}_{frame}(h_{t-1}^f, h_{t+1}^f, f_t).
\]Please refer to Graves et al. \cite{Graves_2013} or the supp. material
for the full equations.

Next we use use a unidirectional LSTM to represent the state of the
event at time $t$:
\[
h_t^e = \mbox{LSTM}(h_{t-1}^e, h_t^f, a_t)
\]
where $a_t$ is a feature vector derived from the players, as we
describe below.
From this, we can predict the class label for the clip using $w_k^T
h_t^e$. We measure the loss as follows:
\begin{equation}
  L =   \frac{1}{2} \sum_{t=1}^T \sum_{k = 1}^K \max (0, 1 - y_k w_k^T h^e_t)^2
\end{equation} 
where $y_k$ is $1$ if the video belongs to class $k$,
else it is $-1$, and the weight vector corresponding to
class $k$ is denoted by $w_k$.

\subsection{Attention models}
\VIGNESH{I have added back my original motivation section for the attention model. I felt
that the section was otherwise very descriptive and didn't motivate the method
sufficiently}

Unlike past attention models \cite{}, we need to attend to a different set of
features at each time-step. There are two key issues to address in this
setting.

First, although we have different detections in each frame, they
can be connected across the frames through an object tracking
method. This could potentially lead to better feature representation of the
players.

Second, player attention depends on the state of the event and needs to evolve
with the event.  For instance, during the start of a ``free-throw" it is
important to attend to the player making the shot. However, towards the end of
the event the success or failure of the shot can be judged by observing the
person in posession of the ball.

With these issues in mind, we first present our model which uses player tracks
and learns a BLSTM based representation for each player track. Next, we also
present a simple tracking-free baseline model.

\noindent \textbf{Attention model with tracking}

We first associate the detecitons
belonging to the same player into tracks using a standard
tracking method. We use a KLT tracker combined with
bipartite graph matching \cite{} to perform the data association.

We use a separate BLSTM to learn a better context based
representation for each player at a given time-step.
The latent representation of player $i$ in frame $t$ is
given by the hidden state
$h_{ti}^p$ of the BLSTM across the player-track:
\[
  h_{ti}^p = \mbox{BLSTM}_{track}(h_{t-1,i}^p, h_{t+1,i}^p, p_{ti}).
\]

To compute $a_t$, we take a convex combination of the player features:
\begin{eqnarray} 
\label{eq:track}
  a_t^{track} & = & \sum_{i=1}^{N_t} \gamma_{ti}^{track} p_{ti} 
\\ \nonumber
  \gamma_{ti}^{track} & = & \text{softmax} \left(\phi\left(h^f_t, h^p_{ti}, h^e_{t-1}\right); \tau\right),
\end{eqnarray}where $N_t$ is the number of detections in frame $t$, and $\phi()$ is a 
multi layer perceptron, similar to \cite{Bahdnau_arxiv14}. $\tau$ is the softmax temperature parameter.

This model is illustrated in Figure~\ref{fig:model}.
Later, we show that this method achieves better
performance at event classification and detection compared to a
tracking-free model. However, it could result in slightly worse ``key player"
identifcation due to tracking errors.

\noindent \textbf{Attention model without tracking}
Here, we treat the detections in each frame to be independent from other
frames.  This alows the model to be more flexible in switching attention
between players as the event progresses.  As we later observe empirically, this
leads to better interpretability.  Also, the model does not suffer from
tracking errors.

We then compute the attention based player feature as shown below:
\begin{eqnarray} 
\label{eq:notrack}
  a_t^{notrack} & = & \sum_{i=1}^{N_t} \gamma_{ti}^{notrack} p_{ti} 
\\ \nonumber
  \gamma_{ti}^{notrack} & = & \text{softmax} \left(\phi\left(h^f_t, p_{ti}, h^e_{t-1}\right); \tau\right),
\end{eqnarray}
